{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "from training import dataset\n",
    "import os\n",
    "import click\n",
    "import tqdm.auto as tqdm    \n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import torch\n",
    "import dnnlib\n",
    "from torch_utils import distributed as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def calculate_inception_stats(\n",
    "    image_path, num_expected=None, seed=0, max_batch_size=64,\n",
    "    num_workers=3, prefetch_factor=2, device=torch.device('cuda'),\n",
    "):\n",
    "    # Rank 0 goes first.\n",
    "    # if dist.get_rank() != 0:\n",
    "    #     torch.distributed.barrier()\n",
    "\n",
    "    # Load Inception-v3 model.\n",
    "    # This is a direct PyTorch translation of http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
    "    print('Loading Inception-v3 model...')\n",
    "    detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl'\n",
    "    detector_kwargs = dict(return_features=True)\n",
    "    feature_dim = 2048\n",
    "    with dnnlib.util.open_url(detector_url, verbose=(dist.get_rank() == 0)) as f:\n",
    "        detector_net = pickle.load(f).to(device)\n",
    "\n",
    "    # List images.\n",
    "    print(f'Loading images from \"{image_path}\"...')\n",
    "    dataset_obj = dataset.ImageFolderDataset(path=image_path, max_size=num_expected, random_seed=seed)\n",
    "    # if num_expected is not None and len(dataset_obj) < num_expected:\n",
    "    #     raise click.ClickException(f'Found {len(dataset_obj)} images, but expected at least {num_expected}')\n",
    "    # if len(dataset_obj) < 2:\n",
    "    #     raise click.ClickException(f'Found {len(dataset_obj)} images, but need at least 2 to compute statistics')\n",
    "\n",
    "    # Other ranks follow.\n",
    "    # if dist.get_rank() == 0:\n",
    "    #     torch.distributed.barrier()\n",
    "\n",
    "    # Divide images into batches.\n",
    "    num_batches = ((len(dataset_obj) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n",
    "    all_batches = torch.arange(len(dataset_obj)).tensor_split(num_batches)\n",
    "    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n",
    "    data_loader = torch.utils.data.DataLoader(dataset_obj, batch_sampler=rank_batches, num_workers=num_workers, prefetch_factor=prefetch_factor)\n",
    "\n",
    "    # Accumulate statistics.\n",
    "    print(f'Calculating statistics for {len(dataset_obj)} images...')\n",
    "    mu = torch.zeros([feature_dim], dtype=torch.float64, device=device)\n",
    "    sigma = torch.zeros([feature_dim, feature_dim], dtype=torch.float64, device=device)\n",
    "    for images, _labels in tqdm.tqdm(data_loader, unit='batch', disable=(dist.get_rank() != 0)):\n",
    "        # torch.distributed.barrier()\n",
    "        if images.shape[0] == 0:\n",
    "            continue\n",
    "        if images.shape[1] == 1:\n",
    "            images = images.repeat([1, 3, 1, 1])\n",
    "        if images.shape[1] == 4:\n",
    "            images = images[:, :3, :, :]\n",
    "        features = detector_net(images.to(device), **detector_kwargs).to(torch.float64)\n",
    "        mu += features.sum(0)\n",
    "        sigma += features.T @ features\n",
    "\n",
    "    # Calculate grand totals.\n",
    "    # torch.distributed.all_reduce(mu)\n",
    "    # torch.distributed.all_reduce(sigma)\n",
    "    mu /= len(dataset_obj)\n",
    "    sigma -= mu.ger(mu) * len(dataset_obj)\n",
    "    sigma /= len(dataset_obj) - 1\n",
    "    return mu.cpu().numpy(), sigma.cpu().numpy()\n",
    "\n",
    "\n",
    "def calculate_inception_stats_from_numpy(\n",
    "    images_np, num_expected=None, seed=0, max_batch_size=64,\n",
    "    num_workers=4, prefetch_factor=2, device=torch.device('cuda'),\n",
    "):\n",
    "    # Load Inception-v3 model.\n",
    "    # This is a direct PyTorch translation of http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
    "    print('Loading Inception-v3 model...')\n",
    "    detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl'\n",
    "    detector_kwargs = dict(return_features=True)\n",
    "    feature_dim = 2048\n",
    "    with dnnlib.util.open_url(detector_url, verbose=False) as f:\n",
    "        detector_net = pickle.load(f).to(device)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    if num_expected is not None:\n",
    "        images_np = images_np[:num_expected]\n",
    "    # NOTE: no normalization, raw uint8 format, [0,1] will yield incorrect results!!!!\n",
    "    # dataset_tensor = torch.from_numpy(images_np).float() / 255.0  # Normalize to [0, 1]\n",
    "    dataset_tensor = torch.from_numpy(images_np)  # no normalization, raw uint8 format\n",
    "    dataset_tensor = dataset_tensor.permute(0, 3, 1, 2)  # Ensure shape (N, 3, H, W)\n",
    "    dataset_tensor = dataset_tensor.to(device)\n",
    "    if dataset_tensor.shape[1] == 1:\n",
    "        dataset_tensor = dataset_tensor.repeat([1, 3, 1, 1])\n",
    "    elif dataset_tensor.shape[1] == 4:\n",
    "        dataset_tensor = dataset_tensor[:, :3, :, :]\n",
    "    assert dataset_tensor.shape[1] == 3\n",
    "    assert dataset_tensor.dtype == torch.uint8\n",
    "    tensor_dataset = TensorDataset(dataset_tensor)\n",
    "    data_loader = DataLoader(\n",
    "        tensor_dataset,\n",
    "        batch_size=max_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "    )\n",
    "    # Accumulate statistics.\n",
    "    print(f'Calculating statistics for {len(tensor_dataset)} images...')\n",
    "    mu = torch.zeros([feature_dim], dtype=torch.float64, device=device)\n",
    "    sigma = torch.zeros([feature_dim, feature_dim], dtype=torch.float64, device=device)\n",
    "    for images, in tqdm.tqdm(data_loader, unit='batch', ):\n",
    "        with torch.no_grad():\n",
    "            features = detector_net(images.to(device), **detector_kwargs).to(torch.float64)\n",
    "        mu += features.sum(0)\n",
    "        sigma += features.T @ features\n",
    "    # Calculate grand totals.\n",
    "    mu /= len(tensor_dataset)\n",
    "    sigma -= mu.ger(mu) * len(tensor_dataset)\n",
    "    sigma /= len(tensor_dataset) - 1\n",
    "    return mu.cpu().numpy(), sigma.cpu().numpy()\n",
    "\n",
    "\n",
    "def calculate_fid_from_inception_stats(mu, sigma, mu_ref, sigma_ref):\n",
    "    m = np.square(mu - mu_ref).sum()\n",
    "    s, _ = scipy.linalg.sqrtm(np.dot(sigma, sigma_ref), disp=False)\n",
    "    fid = m + np.trace(sigma + sigma_ref - s * 2)\n",
    "    return float(np.real(fid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main computation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Datasets/EDM_datasets/fid-refs\"\n",
    "refdata_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Datasets/EDM_datasets/datasets\"\n",
    "sample_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/edm_analy_sampler_benchmark/samples/\"\n",
    "eval_root = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/edm_analy_sampler_benchmark/eval/\"\n",
    "model_ckpt_dict = {\"afhqv264\": \"edm-afhqv2-64x64-uncond-vp\",\n",
    "                   \"ffhq64\": \"edm-ffhq-64x64-uncond-vp\",\n",
    "                   \"cifar10\": \"edm-cifar10-32x32-uncond-vp\"}\n",
    "refdata_dict = {\"afhqv264\": \"afhqv2-64x64.zip\",\n",
    "                \"ffhq64\": \"ffhq-64x64.zip\",\n",
    "                \"cifar10\": \"cifar10-32x32.zip\"}\n",
    "refstats_dict = {\"afhqv264\": \"afhqv2-64x64.npz\",\n",
    "                \"ffhq64\": \"ffhq-64x64.npz\",\n",
    "                \"cifar10\": \"cifar10-32x32.npz\"}\n",
    "refstats_url_dict = {\"ffhq64\": \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/ffhq-256.npz\",\n",
    "            \"afhqv264\": \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/afhqv2-64x64.npz\",\n",
    "            \"cifar10\": \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating dpm_solver++_10_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8be277aacb4beabdb0499be3a79b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_10_skip1.0 FID: 29.34\n",
      "Evaluating dpm_solver++_10_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e444a50a748c4782bbb7a340bde8a4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_10_skip10.0 FID: 2.94\n",
      "Evaluating dpm_solver++_10_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095be8f9e5054cbb9bb6bd8943751ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_10_skip2.5 FID: 7.23\n",
      "Evaluating dpm_solver++_10_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08302a190d9b45fcbcda53e9ed7227c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_10_skip20.0 FID: 2.84\n",
      "Evaluating dpm_solver++_10_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831b475d6dd84f7ba8a1dcf2fea5702e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_10_skip40.0 FID: 2.83\n",
      "Evaluating dpm_solver++_10_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32340e7b479c49d88a08cdd760dfb0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_10_skip5.0 FID: 3.62\n",
      "Evaluating dpm_solver++_10_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3287697949434c7baf16d5ef1a448223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_10_skip80.0 FID: 3.06\n",
      "Evaluating dpm_solver++_12_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c3de4fe16244ddb9b2663c14ecb05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_12_skip1.0 FID: 28.28\n",
      "Evaluating dpm_solver++_12_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aba7d7224b3413cbf130e1201196aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_12_skip10.0 FID: 2.44\n",
      "Evaluating dpm_solver++_12_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c2286fc07b4a9191be31405461d494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_12_skip2.5 FID: 6.28\n",
      "Evaluating dpm_solver++_12_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1474d38469d74b648206b5926998a560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_12_skip20.0 FID: 2.36\n",
      "Evaluating dpm_solver++_12_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd9177cf2064969a2c5f477e240ead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_12_skip40.0 FID: 2.44\n",
      "Evaluating dpm_solver++_12_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f0ed69e0744e2a92253f947b6221cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_12_skip5.0 FID: 2.96\n",
      "Evaluating dpm_solver++_12_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90828420c0414c01a126205c171f2757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_12_skip80.0 FID: 2.58\n",
      "Evaluating dpm_solver++_15_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2433b758a40048a8bd25afd90e6684e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_15_skip1.0 FID: 27.39\n",
      "Evaluating dpm_solver++_15_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561b5df737454bd0826042ab5c8e2634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_15_skip10.0 FID: 2.15\n",
      "Evaluating dpm_solver++_15_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4deb7644cf647f690a8f1fdfa858c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_15_skip2.5 FID: 5.59\n",
      "Evaluating dpm_solver++_15_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d54c893b154b5b86009577150dd68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_15_skip20.0 FID: 2.13\n",
      "Evaluating dpm_solver++_15_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccd7d4f38af4556a6f6c15346e76b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_15_skip40.0 FID: 2.19\n",
      "Evaluating dpm_solver++_15_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497007a175bd4485b4a0d6179701f99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_15_skip5.0 FID: 2.56\n",
      "Evaluating dpm_solver++_15_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0b374d96a24d2581411d64c81b5a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_15_skip80.0 FID: 2.26\n",
      "Evaluating dpm_solver++_20_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d58dd6b8164e4384caf6dcd4624eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_20_skip1.0 FID: 26.62\n",
      "Evaluating dpm_solver++_20_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c7f39c0aea4bb0911aa7d6bb8c9db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_20_skip10.0 FID: 2.05\n",
      "Evaluating dpm_solver++_20_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708a88b12f914bf8947120bdeedbac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_20_skip2.5 FID: 5.09\n",
      "Evaluating dpm_solver++_20_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98855a63be1c47a986b519569619b071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_20_skip20.0 FID: 2.05\n",
      "Evaluating dpm_solver++_20_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea182bc4d0c4e0bad567578e22bac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_20_skip40.0 FID: 2.09\n",
      "Evaluating dpm_solver++_20_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a98496060e471e980a823ff8556219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_20_skip5.0 FID: 2.34\n",
      "Evaluating dpm_solver++_20_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b87842ac244673accf703ea3481294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_20_skip80.0 FID: 2.12\n",
      "Evaluating dpm_solver++_25_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8a2274b08545dba9f9558ee64c8bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_25_skip1.0 FID: 26.22\n",
      "Evaluating dpm_solver++_25_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124a11ad9e9c4cda883b3ba25c699c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_25_skip10.0 FID: 2.03\n",
      "Evaluating dpm_solver++_25_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3c5133ed22425380ccb756c02bbda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_25_skip2.5 FID: 4.86\n",
      "Evaluating dpm_solver++_25_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf47b63a09a4d82a554f87849ce3637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_25_skip20.0 FID: 2.04\n",
      "Evaluating dpm_solver++_25_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb80ca9ddba5443eb6cee90d65337c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_25_skip40.0 FID: 2.08\n",
      "Evaluating dpm_solver++_25_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b69a2ef88f148b9a7ad7bb525833dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_25_skip5.0 FID: 2.27\n",
      "Evaluating dpm_solver++_25_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3893ee5a5ae43b3a75b241eaac90b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_25_skip80.0 FID: 2.09\n",
      "Evaluating dpm_solver++_5_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a6931c4ee74bb28945a8283c9b7c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_5_skip1.0 FID: 34.46\n",
      "Evaluating dpm_solver++_5_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cba295cf31a47cdab06012ac4dfd536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_5_skip10.0 FID: 12.27\n",
      "Evaluating dpm_solver++_5_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d450fb82140341a2895cdf3901192ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_5_skip2.5 FID: 14.25\n",
      "Evaluating dpm_solver++_5_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66b7beef2e145d49e64d526e0f12614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_5_skip20.0 FID: 14.37\n",
      "Evaluating dpm_solver++_5_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa86027a2b54e6f804c39f0bdbce967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_5_skip40.0 FID: 18.29\n",
      "Evaluating dpm_solver++_5_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0925323d1b3d476b8f68eb12f348c235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_5_skip5.0 FID: 11.37\n",
      "Evaluating dpm_solver++_5_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db25ddce3474d409c7f9b42e01b7eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_5_skip80.0 FID: 25.11\n",
      "Evaluating dpm_solver++_6_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3486d43c00480e9a786b05f1f50732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_6_skip1.0 FID: 34.26\n",
      "Evaluating dpm_solver++_6_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec067d4c4ca24870a252a93a5eb67701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_6_skip10.0 FID: 6.78\n",
      "Evaluating dpm_solver++_6_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202cc7a16a164f4c9b83e9459e769e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_6_skip2.5 FID: 11.79\n",
      "Evaluating dpm_solver++_6_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b6611c45de4b4aa5fa1d9f34c87ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_6_skip20.0 FID: 7.22\n",
      "Evaluating dpm_solver++_6_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cd1ef67a714394a1e23bd8d2ec8e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_6_skip40.0 FID: 9.00\n",
      "Evaluating dpm_solver++_6_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a48726e98f349d1abed073b7555e713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_6_skip5.0 FID: 7.40\n",
      "Evaluating dpm_solver++_6_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba872b522bc4c8cb2ed55cf1a298517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_6_skip80.0 FID: 12.23\n",
      "Evaluating dpm_solver++_8_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465e082af0ea48d69355a7cc5fac5033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_8_skip1.0 FID: 31.50\n",
      "Evaluating dpm_solver++_8_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac212d4e1594cf2b8c57834bdf34a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_8_skip10.0 FID: 3.96\n",
      "Evaluating dpm_solver++_8_skip2.5...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1a21de36b34e2caee8011525dca06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_8_skip2.5 FID: 9.13\n",
      "Evaluating dpm_solver++_8_skip20.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8e101639124bf39bb47a945eb1fad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_8_skip20.0 FID: 3.71\n",
      "Evaluating dpm_solver++_8_skip40.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf2c81639404112b37b14f434514bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_8_skip40.0 FID: 3.82\n",
      "Evaluating dpm_solver++_8_skip5.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a6fbda32a24490b41830989f3dda4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_8_skip5.0 FID: 4.93\n",
      "Evaluating dpm_solver++_8_skip80.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23237bb95024a9eb415c0ef85b0b51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver++_8_skip80.0 FID: 4.56\n",
      "Evaluating dpm_solver_v3_10_skip1.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d3cfadcfbc4e849d061fd2c40dcaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver_v3_10_skip1.0 FID: 31.93\n",
      "Evaluating dpm_solver_v3_10_skip10.0...\n",
      "(50176, 32, 32, 3)\n",
      "Loading Inception-v3 model...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083f9abaf3584312ac96c7fa075b2a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m sample_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39mload(f)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m npz_files], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (N, 3, H, W) uint8 format\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample_all\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 27\u001b[0m Mu, Sigma \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_inception_stats_from_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_expected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefetch_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(Mu.shape, Sigma.shape)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m fid \u001b[38;5;241m=\u001b[39m calculate_fid_from_inception_stats(Mu, Sigma, ref_mu, ref_sigma)\n",
      "Cell \u001b[0;32mIn[13], line 118\u001b[0m, in \u001b[0;36mcalculate_inception_stats_from_numpy\u001b[0;34m(images_np, num_expected, seed, max_batch_size, num_workers, prefetch_factor, device)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(data_loader, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m, ):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 118\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[43mdetector_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdetector_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    119\u001b[0m     mu \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    120\u001b[0m     sigma \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m features\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<string>:213\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, img, return_features, use_fp16, no_output_bias)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<string>:160\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<string>:20\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/nn/functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_name = \"cifar10\" # \"ffhq64\" # \"cifar10\"  #\n",
    "\n",
    "model_ckpt = model_ckpt_dict[dataset_name]\n",
    "model_dir = join(sample_root, model_ckpt)\n",
    "eval_dir = join(eval_root, model_ckpt)\n",
    "os.makedirs(eval_root, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "ref = np.load(join(fid_root, refstats_dict[dataset_name]))\n",
    "ref_mu, ref_sigma = ref[\"mu\"], ref[\"sigma\"]\n",
    "# with dnnlib.util.open_url(refstats_url_dict[dataset_name]) as f:\n",
    "#     ref = dict(np.load(f))\n",
    "#     ref_mu, ref_sigma = ref[\"mu\"], ref[\"sigma\"]\n",
    "\n",
    "\"\"\"for all subfolders in model_dir, collect all files ending with .npz, load them as numpy arrays, and concatenate them along the first axis\"\"\"\n",
    "fid_col = []\n",
    "# for subdir in [\"uni_pc_bh1_25_skip20.0\"]:#[\"dpm_solver_v3_25_skip80.0\"]:#tqdm.tqdm(os.listdir(model_dir)):\n",
    "for subdir in sorted(os.listdir(model_dir)):\n",
    "    sampler_str = subdir \n",
    "    print(f\"Evaluating {sampler_str}...\")\n",
    "    npz_files = [join(model_dir, subdir, f) for f in os.listdir(join(model_dir, subdir)) if f.endswith(\".npz\")]\n",
    "    if len(npz_files) == 0:\n",
    "        print(f\"No npz files found in {model_dir}/{subdir}\")\n",
    "        continue\n",
    "    sample_all = np.concatenate([np.load(f)[\"samples\"] for f in npz_files], axis=0) # shape (N, 3, H, W) uint8 format\n",
    "    print(sample_all.shape)\n",
    "    Mu, Sigma = calculate_inception_stats_from_numpy(sample_all, num_expected=50000,\n",
    "                                   seed=0, max_batch_size=512, num_workers=0, prefetch_factor=None,\n",
    "                                   device=torch.device('cuda'))\n",
    "    # print(Mu.shape, Sigma.shape)\n",
    "    fid = calculate_fid_from_inception_stats(Mu, Sigma, ref_mu, ref_sigma)\n",
    "    print(f\"{sampler_str} FID: {fid:.2f}\")\n",
    "    # save Mu, Sigma, fid to eval_dir\n",
    "    np.savez(join(eval_dir, f\"{sampler_str}_stats.npz\"), mu=Mu, sigma=Sigma, fid=fid)\n",
    "    fid_col.append({\"sampler\": sampler_str, \"fid\": fid, \"dataset\": dataset_name, }) # \"ckpt\": model_ckpt, \n",
    "    # save fid_col to eval_dir\n",
    "    pd.DataFrame(fid_col).to_csv(join(eval_dir, \"fid_by_sampler.csv\"))\n",
    "    # raise ValueError(\"stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni_pc_bh1_25_skip20.0 FID: 2.0426\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sampler_str} FID: {fid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_1 = \"uni_pc_bh1_20_skip80.0\"\n",
    "sampler_2 = \"dpm_solver_v3_20_skip80.0\"\n",
    "sampler_2 = \"uni_pc_bh2_20_skip10.0\"\n",
    "\n",
    "data1 = np.load(join(eval_dir, f\"{sampler_1}_stats.npz\"))\n",
    "data2 = np.load(join(eval_dir, f\"{sampler_2}_stats.npz\"))\n",
    "calculate_fid_from_inception_stats(data1[\"mu\"], data1[\"sigma\"], data2[\"mu\"], data2[\"sigma\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note original code, input format is 255 uint8. No normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "refdata_path = join(refdata_root, refdata_dict[dataset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_obj = dataset.ImageFolderDataset(path=refdata_path, max_size=50000, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 59,  43,  50, ..., 158, 152, 148],\n",
       "         [ 16,   0,  18, ..., 123, 119, 122],\n",
       "         [ 25,  16,  49, ..., 118, 120, 109],\n",
       "         ...,\n",
       "         [208, 201, 198, ..., 160,  56,  53],\n",
       "         [180, 173, 186, ..., 184,  97,  83],\n",
       "         [177, 168, 179, ..., 216, 151, 123]],\n",
       " \n",
       "        [[ 62,  46,  48, ..., 132, 125, 124],\n",
       "         [ 20,   0,   8, ...,  88,  83,  87],\n",
       "         [ 24,   7,  27, ...,  84,  84,  73],\n",
       "         ...,\n",
       "         [170, 153, 161, ..., 133,  31,  34],\n",
       "         [139, 123, 144, ..., 148,  62,  53],\n",
       "         [144, 129, 142, ..., 184, 118,  92]],\n",
       " \n",
       "        [[ 63,  45,  43, ..., 108, 102, 103],\n",
       "         [ 20,   0,   0, ...,  55,  50,  57],\n",
       "         [ 21,   0,   8, ...,  50,  50,  42],\n",
       "         ...,\n",
       "         [ 96,  34,  26, ...,  70,   7,  20],\n",
       "         [ 96,  42,  30, ...,  94,  34,  34],\n",
       "         [116,  94,  87, ..., 140,  84,  72]]], dtype=uint8),\n",
       " array([], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_obj[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug FID feature calculate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Inception-v3 model...\n",
      "Loading images from \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Datasets/EDM_datasets/datasets/cifar10-32x32.zip\"...\n",
      "Calculating statistics for 50000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241e0ae88d6f4f8ca21b4a5f37ea57fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.7743751329426898e-05"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ref data\n",
    "refdata_path = join(refdata_root, refdata_dict[dataset_name])\n",
    "ref_mu_new, ref_sigma_new = calculate_inception_stats(refdata_path, num_expected=50000,\n",
    "                                   seed=0, max_batch_size=512, num_workers=0, prefetch_factor=None,\n",
    "                                   device=torch.device('cuda'))\n",
    "#%%\n",
    "# fiddata = np.load(join(fid_root, refstats_dict[dataset_name]))\n",
    "with dnnlib.util.open_url(refstats_url_dict[dataset_name]) as f:\n",
    "    ref = dict(np.load(f))\n",
    "    ref_mu, ref_sigma = ref[\"mu\"], ref[\"sigma\"]\n",
    "#%%\n",
    "calculate_fid_from_inception_stats(ref_mu_new, ref_sigma_new, ref_mu, ref_sigma) # 2.77 E-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug the feature network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl'\n",
    "detector_kwargs = dict(return_features=True)\n",
    "feature_dim = 2048\n",
    "with dnnlib.util.open_url(detector_url, verbose=False) as f:\n",
    "    detector_net = pickle.load(f).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug the dataset formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_np = sample_all\n",
    "dataset_tensor = torch.from_numpy(images_np).float() / 255.0  # Normalize to [0, 1]\n",
    "dataset_tensor = dataset_tensor.permute(0, 3, 1, 2)  # Ensure shape (N, 3, H, W)\n",
    "dataset_tensor = dataset_tensor.to(\"cuda\")\n",
    "if dataset_tensor.shape[1] == 1:\n",
    "    dataset_tensor = dataset_tensor.repeat([1, 3, 1, 1])\n",
    "elif dataset_tensor.shape[1] == 4:\n",
    "    dataset_tensor = dataset_tensor[:, :3, :, :]\n",
    "assert dataset_tensor.shape[1] == 3\n",
    "tensor_dataset = TensorDataset(dataset_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "def find_unique_suffixes(folder_name):\n",
    "    # List all the files in the specified folder\n",
    "    file_names_list = [f for f in os.listdir(folder_name) if os.path.isfile(os.path.join(folder_name, f))]\n",
    "\n",
    "    # Regular expression pattern to match the \"skipXX_noiseXX\" part\n",
    "    pattern = re.compile(r'skip\\d+_noise\\d+')\n",
    "\n",
    "    # Extract the specific suffix pattern from each file name stem\n",
    "    suffixes = [pattern.search(file_name.split('.')[0]).group() for file_name in file_names_list if\n",
    "                pattern.search(file_name.split('.')[0])]\n",
    "\n",
    "    # Get unique suffixes\n",
    "    unique_suffixes = set(suffixes)\n",
    "\n",
    "    # Define a function to extract the number after \"skip\"\n",
    "    def get_skip_number(suffix):\n",
    "        return int(suffix.split('_')[0].replace('skip', ''))\n",
    "\n",
    "    # Sort the unique suffixes by the number after \"skip\"\n",
    "    sorted_suffixes = sorted(unique_suffixes, key=get_skip_number)\n",
    "\n",
    "    return sorted_suffixes\n",
    "\n",
    "\n",
    "def crop_all_from_montage(img, totalnum, imgsize=32, pad=2):\n",
    "    \"\"\"Return all crops from a montage image\"\"\"\n",
    "    nrow, ncol = (img.shape[0] - pad) // (imgsize + pad), (img.shape[1] - pad) // (imgsize + pad)\n",
    "    imgcol = []\n",
    "    for imgid in range(totalnum):\n",
    "        ri, ci = np.unravel_index(imgid, (nrow, ncol))\n",
    "        img_crop = img[pad + (pad + imgsize) * ri:pad + imgsize + (pad + imgsize) * ri, \\\n",
    "               pad + (pad + imgsize) * ci:pad + imgsize + (pad + imgsize) * ci, :]\n",
    "        imgcol.append(img_crop)\n",
    "    return imgcol\n",
    "\n",
    "\n",
    "def find_files_with_suffix(folder_path, target_suffix):\n",
    "    # List all files in the specified folder\n",
    "    all_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    # Filter files that contain the target suffix\n",
    "    matching_files = sorted([file_name for file_name in all_files if target_suffix in file_name])\n",
    "\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabdir = r\"E:\\OneDrive - Harvard University\\NeurIPS2023_Diffusion\\Tables\"\n",
    "tabdir = r\"D:\\DL_Projects\\Vision\\edm_analy_sample\\summary\"\n",
    "tabdir = r\"/n/scratch3/users/b/biw905/edm_analy_sample/summary\"\n",
    "tabdir = r\"/home/binxu/DL_Projects/edm_analy_sample/summary\"\n",
    "imgsize_dict = {\"ffhq64\": 64, \"afhqv264\": 64, \"cifar10\": 32, }\n",
    "max_batch_size_dict = {\"ffhq64\": 64, \"afhqv264\": 64, \"cifar10\": 256, }\n",
    "# refstats_dict = {\"ffhq64\": \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/ffhq-256.npz\",\n",
    "#             \"afhqv264\": \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/afhqv2-64x64.npz\",\n",
    "#             \"cifar10\": \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\"}\n",
    "\n",
    "# refstats_url = r\"ffhq-64x64.npz\"\n",
    "refstats_dict = {#\"ffhq64\": \"ffhq-256.npz\",\n",
    "                \"ffhq64\": \"ffhq-64x64.npz\",\n",
    "                \"afhqv264\": \"afhqv2-64x64.npz\",\n",
    "                \"cifar10\": \"cifar10-32x32.npz\"}\n",
    "\n",
    "\n",
    "dataset_name = \"ffhq64\" # \"afhqv264\"  # \"ffhq64\" # \"cifar10\"  #\n",
    "figdir = rf\"D:\\DL_Projects\\Vision\\edm_analy_sample\\{dataset_name}_uncond_vp_edm_theory\"\n",
    "figdir = rf\"/n/scratch3/users/b/biw905/edm_analy_sample/{dataset_name}_uncond_vp_edm_theory\"\n",
    "figdir = rf\"/home/binxu/DL_Projects/edm_analy_sample/{dataset_name}_uncond_vp_edm_theory\"\n",
    "croproot = figdir + \"_crops\"\n",
    "imgsize = imgsize_dict[dataset_name]\n",
    "max_batch_size = max_batch_size_dict[dataset_name]\n",
    "refstats_url = refstats_dict[dataset_name]\n",
    "\n",
    "suffixes = find_unique_suffixes(figdir)\n",
    "for suffix in suffixes:\n",
    "    os.makedirs(join(croproot, suffix), exist_ok=True)\n",
    "os.makedirs(tabdir, exist_ok=True)\n",
    "#%%\n",
    "# load all mtg figures crop and save into folders\n",
    "for suffix in suffixes:\n",
    "    mtglist = find_files_with_suffix(figdir, suffix)\n",
    "    for mtgname in tqdm.tqdm(mtglist):\n",
    "        numbers_before_after_dash = re.findall(r'rnd(\\d+)-(\\d+)_', mtgname)\n",
    "        rnd_start, rnd_end = numbers_before_after_dash[0]\n",
    "        rnd_batch = list(range(int(rnd_start), int(rnd_end) + 1))\n",
    "        mtg_arr = plt.imread(join(figdir, mtgname))\n",
    "        imgcrops = crop_all_from_montage(mtg_arr, max_batch_size, imgsize=imgsize, pad=2)\n",
    "        for imgcrop, rnd_id in zip(imgcrops, rnd_batch):\n",
    "            plt.imsave(join(croproot, suffix, f\"rnd{rnd_id:06d}.png\"), imgcrop)\n",
    "\n",
    "#%\n",
    "fid_batch_size = 256\n",
    "fid_col = []\n",
    "for suffix in suffixes:\n",
    "    Mu, Sigma = calculate_inception_stats(join(croproot, suffix), num_expected=50000,\n",
    "                                   seed=0, max_batch_size=fid_batch_size, num_workers=0, prefetch_factor=None,\n",
    "                                   device=torch.device('cuda'))\n",
    "    # https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\n",
    "    with dnnlib.util.open_url(refstats_url) as f:\n",
    "        ref = dict(np.load(f))\n",
    "\n",
    "    fid = calculate_fid_from_inception_stats(Mu, Sigma, ref['mu'], ref['sigma'])\n",
    "    print(f\"{suffix} FID: {fid:.2f}\")\n",
    "    fid_col.append(fid)\n",
    "#%%\n",
    "# sorted(os.listdir(croproot))\n",
    "# with the folder name column\n",
    "df = pd.DataFrame(fid_col, columns=[\"FID\"], index=suffixes)\n",
    "# df.to_csv(join(croproot, \"fid_by_skipping.csv\"))\n",
    "df.to_csv(join(tabdir, f\"{dataset_name}_fid_by_skipping_new.csv\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DEV ZONE\n",
    "\n",
    "# TODO: find a way to compute t_steps from skipsteps\n",
    "# suffixes = find_unique_suffixes(r\"D:\\DL_Projects\\Vision\\edm_analy_sample\\ffhq64_uncond_vp_edm_theory\")\n",
    "# # for skipstep in [0, 1, 2, 3, 4, 5, 6, 7, 8, ]:  # range(1, num_steps):\n",
    "# #     sigma_max_skip = t_steps[skipstep]\n",
    "# #     print(f\"skip{skipstep}_noise{sigma_max_skip:.0f}\")\n",
    "# filenames = find_files_with_suffix(r\"D:\\DL_Projects\\Vision\\edm_analy_sample\\ffhq64_uncond_vp_edm_theory\",\n",
    "#                                       suffixes[0])\n",
    "# #%%\n",
    "# figdir = \"/home/binxu/DL_Projects/edm_analy_sample/cifar10_uncond_vp_edm\"\n",
    "# croproot = \"/home/binxu/DL_Projects/edm_analy_sample/cifar10_uncond_vp_edm_crops\"\n",
    "\n",
    "#%%\n",
    "# load all mtg figures crop and save into folders\n",
    "seeds = list(range(50000))\n",
    "max_batch_size = 256\n",
    "# iterate batches\n",
    "rank_batches = np.array_split(seeds, len(seeds) // max_batch_size)\n",
    "for batch_seeds in rank_batches:\n",
    "    for skip, sigma_max_skip in zip([0, 1, 2, 3, 4, 5, 6, 7, 8, ],\n",
    "                                    t_steps):\n",
    "        assert os.path.exists(join(figdir, f\"rnd{batch_seeds[0]:06d}-{batch_seeds[-1]:06d}\"\n",
    "                                           f\"_skip{skip}_noise{sigma_max_skip:.0f}.png\"))\n",
    "        mtg_arr = plt.imread(join(figdir, f\"rnd{batch_seeds[0]:06d}-{batch_seeds[-1]:06d}\"\n",
    "                                             f\"_skip{skip}_noise{sigma_max_skip:.0f}.png\"))\n",
    "        imgcrops = crop_all_from_montage(mtg_arr, len(batch_seeds), imgsize=32, pad=2)\n",
    "        for imgid, imgcrop in enumerate(imgcrops):\n",
    "            plt.imsave(join(croproot, f\"skip{skip}_noise{sigma_max_skip:.0f}\", f\"rnd{batch_seeds[imgid]:06d}.png\"), imgcrop)\n",
    "\n",
    "#%%\n",
    "croproot = \"/home/binxu/DL_Projects/edm_analy_sample/cifar10_uncond_vp_edm_crops\"\n",
    "foldername = f\"skip1_noise58\"\n",
    "#%%\n",
    "# from fid import calculate_inception_stats\n",
    "# skip, sigma_max_skip = 0, t_steps[0]\n",
    "fid_col = []\n",
    "for foldername in sorted(os.listdir(croproot)):\n",
    "    Mu, Sigma = calculate_inception_stats(join(croproot, foldername), num_expected=50000,\n",
    "                                   seed=0, max_batch_size=256, num_workers=0, prefetch_factor=None,\n",
    "                                device=torch.device('cuda'))\n",
    "\n",
    "    with dnnlib.util.open_url(\"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\") as f:\n",
    "        ref = dict(np.load(f))\n",
    "\n",
    "    fid = calculate_fid_from_inception_stats(Mu, Sigma, ref['mu'], ref['sigma'])\n",
    "    print(f\"{foldername} FID: {fid:.2f}\")\n",
    "    fid_col.append(fid)\n",
    "#%%\n",
    "# tabdir = r\"E:\\OneDrive - Harvard University\\NeurIPS2023_Diffusion\\Tables\"\n",
    "tabdir = r\"/home/binxu/DL_Projects/edm_analy_sample/summary\"\n",
    "\n",
    "# sorted(os.listdir(croproot))\n",
    "# with the folder name column\n",
    "df = pd.DataFrame(fid_col, columns=[\"FID\"], index=sorted(os.listdir(croproot)))\n",
    "# df.to_csv(join(croproot, \"fid_by_skipping.csv\"))\n",
    "df.to_csv(join(tabdir, \"fid_by_skipping.csv\"))\n",
    "\n",
    "\n",
    "#%%\n",
    "#%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
